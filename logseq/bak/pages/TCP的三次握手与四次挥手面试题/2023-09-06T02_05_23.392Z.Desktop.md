- # TCP基础
	- **TCP是[[$red]]==面向连接，可靠的，字节流==传输**
	- ## TCP头格式
		- ![TCP 头格式](https://cdn.xiaolincoding.com//mysql/other/format,png-20230309230534096.png)
	- ## TCP连接的本质
	  collapsed:: true
		- 是一个**状态机**这个状态机需要维护某些状态信息，这些信息包括**Socket，序列号和窗口大小**
		- ![](https://cdn.xiaolincoding.com//mysql/other/format,png-20230309230428466.png)
		- **Socket**包括IP地址和端口号
		- **序列号**用于解决乱序问题
		- **窗口大小**用于流量控制
		- ### 唯一确定一个TCP连接的[[$red]]==要素==
		  collapsed:: true
			- 一个**四元组**即可在整个互联网上唯一确定一个TCP连接，包括：
				- **源地址**
				- **源端口**
				- **目标地址**
				- **目标端口**
		- ### 服务器端的最大TCP连接数
		  collapsed:: true
			- **[[$red]]==最大TCP连接数\= 客户端IP数 * 客户端端口数==**
			- 对于IPv4来说，客户端IP最多为``2^32``，客户端端口数最多为``2^16``，因此服务器端单机最大TCP连接数为``2^48``
			- 然而实际上理论值往往都无法达到，实际上服务器端最大TCP连接数受以下因素影响：
				- **文件描述符限制**
					- 每个TCP连接在Linux看来都是一个文件，如果文件描述符占满，会触发``Too many open files``
					- Linux对可打开文件描述符数量做了三方面限制
						- **系统级**：当前系统可以打开的最大数量，可通过``/proc/sys/fs/file-max``文件查看
						- **用户级**：指定用户可以打开的最大数量，可通过``etc/security/limits.conf``文件查看
							- 条目中nofile选项即为规定的最大可打开文件数量
						- **进程级**：单个进程可打开的最大数量，通过``/proc/sys/fs/nr_open``文件查看
				- **内存限制**
					- 每个TCP连接都会占用一定内存，内存资源占满之后会发生OOM
- # TCP连接建立
	- ![TCP 三次握手](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4/%E7%BD%91%E7%BB%9C/TCP%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B.drawio.png){:height 419, :width 489}
	- 三次握手本质上是为了**交换窗口大小和序列号信息**
	- 任何一方在得知对方的初始序列号和窗口大小之后，**都可以开始发送信息**
	- 三次握手完成之后，连接即处于**ESTABLISHED**状态
	- ## 查看TCP连接状态
		- Linux中可以通过``netstat -napt``命令查看所有连接状态
		- ![TCP 连接状态查看](https://cdn.xiaolincoding.com//mysql/other/format,png-20230309230520683.png)
	- ## 握手为什么一定要是三次
		- 三次握手才能**初始化Socket，序列号和窗口大小**
		- ### 避免历史连接
		  collapsed:: true
			- 三次握手可以**防止旧有重复连接造成的初始化混乱**
			- 如果客户端第一次企图建立连接，发送了一个序列号为90的sync帧，发送之后客户端发生了重启，重启之后发送了一个序列号为100的sync帧，三次握手便可以避免建立一次错误的历史连接，而两次握手不行
				- 原因在于，三次握手中服务器端在收到客户端的sync帧后有一个中间状态，这个中间状态避免了历史连接的建立
			- 三次握手的情况
				- ![三次握手避免历史连接](https://cdn.xiaolincoding.com//mysql/other/format,png-20230309230525514.png){:height 567, :width 382}
			- 两次握手的情况
				- ![两次握手无法阻止历史连接](https://cdn.xiaolincoding.com//mysql/other/fe898053d2e93abac950b1637645943f.png){:height 456, :width 436}
		- ### 同步双方初始序列号和窗口大小
		  collapsed:: true
			- 序列号在TCP传输中**十分重要**，通过序列号可以做到：
				- 接收方可以去除重复数据
				- 接收方可以根据序列号按序接受数据
				- 通过ACK帧的ACK序列号可以标识已发送数据包中已被对方接收到的部分
			- 两次握手只能保证其中一方接收到了对方的初始序列号和窗口大小
			- 四次握手冗余，其中有多余的，可以被优化的步骤
		- ### 避免资源浪费
		  collapsed:: true
			- 假设一种没有第三次握手(即不需要客户端ACK，服务器端发送SYN帧之后立即建立连接)
			- 由于服务器端无法知晓对方是否收到自己的sync帧，因此若在客户端的数据帧到来之前，一个历史sync帧到来(上帝视角能够知道是历史sync帧，服务器端无法判别)，服务器无法知道这是因为对方没有收到sync帧导致超时关闭连接后试图重新建立连接还是一个历史sync帧，因此只能先选择建立一个新的连接，等到后续客户端的rst帧确认应该关闭哪一个连接
			- 无论如何，这都导致资源的浪费
			- ![两次握手会造成资源浪费](https://cdn.xiaolincoding.com//mysql/other/format,png-20230309230636571.png){:height 514, :width 306}
	- ## 初始化序列号为何每次必须随机选择
		- 若每次序列号都相同，将**无法很好地区分网络中延迟到达的历史网络包和连接实际需要的包**
		- 若每次都选择随机序列号，讲有和大概率可以避免历史数据包**[[$red]]==不落在期望接受窗口中==**从而避免历史包对于通信的干扰
		- 当然只是，大概率，**无法完全避免历史包干扰通信事件的发生**
		- ### 初始序列号(ISN)如何产生
			- 初始ISN的产生**基于时钟**。
			- RFC793踢刀初始化序列号的随机生成算法为：``ISN = M + F(localhost, localport, remotehost, remoteport)``
				- 其中，M为一个计时器，此计时器每个4微秒加一
				- F是一个Hash算法，根据源IP，目的IP，源端口，目的端口生成一个随机数值，可能使用MD5算法
	- ## 握手消息丢失的后果
		- ### 丢失第一次握手
			- #### 对于客户端来说
				- 一般在建立连接时，由客户端发送的第一次握手的syn帧若迟迟没有收到服务器的ack，客户端会在超市之后重传此帧(帧的数据保持一致)
					- 超时时间是内核固定写死的，不可改变
					- 重试次数可以由``/proc/sys/net/ipv4/tcp_syn_retries``文件控制，一般默认为5
				- 一般第一次超时重传为1秒，第二次是2秒，第三次为4秒，之后以此以2倍递增
				- **到达重传上线后**，会继续等待上一次超时的两倍时间，若期间仍未收到ACK则**断开连接**
		- ### 丢失第二次握手
			- 第二次握手消息有两个用途
				- 对第一次握手ACK
				- 告知对方自己的序列号和窗口大小
			- 如果第二次握手消息丢失，客户端会出发超时重传，重传第一次SYN报文
				- 服务器端也可能因为超时重传，重传SYN-ACK报文
			- 在linux中，SYN-ACK报文由``/proc/sys/net/ipv4/tcp_synack_retries``确定
		- ### 丢失第三次握手
			- 第三次握手实际上是对SYN-ACK消息的ACK，且一般已经带有数据
			- ACK报文不会自己重传，需要由对方通过重传请求，也就是服务器会在收到第三次握手消息前不停重发SYN-ACK帧，知道收到这个ACK，若重传超过系统设置，则关闭连接
- # MAC帧的MTU和TCP的MSS
	- ![MTU 与 MSS](https://cdn.xiaolincoding.com//mysql/other/format,png-20230309230633447.png)
	- **MTU(Maximum Trasmission Unit)**是一个硬件包的最大长度，也是基于该硬件传输的所有网络包的最大长度，以太网中一般为**1500字节**
	- **MSS(Maximum Segment Size)**是指一个TCP数据包中所能容纳的最大TCP长度，即最大分段长度
	- 若IP层有超过MTU大小的数据，**IP层需要讲数据进行分片**，保证每一片分片的长度都小于MTU，不然无法塞入链路层的帧中
	- 但是IP分片会导致某些问题：
		- **若一个IP分片丢失，那么对于传输层来说，整个IP报文的所有分片都需要重传**
		- 由于IP本身并没有超时重传机制，超时重传需要通过TCP负责
	- 因此TCP为了增加重传效率，设置了自己的最大长度MSS，MSS一般通过双方协商确定
	- 若设定了MSS，则当超时重传发生之后，TCP层的重传将只会涉及到本层的一次TCP段重传，增加重传效率
- # SYN攻击和其避免方式
	- ![SYN 攻击](https://cdn.xiaolincoding.com//mysql/other/format,png-20230309230625853.png){:height 213, :width 300}
	- 攻击者通过不停发送SYN帧，则服务器不得不建立很多半连接，最终导致**[[$red]]==占满服务器半连接队列==**，使得服务器瘫痪
	- Linux内核会**维护两个队列**，分别是
		- **半连接队列**，也称**SYN队列**
		- **全连接队列**，也称**accept队列**
	- ![正常流程](https://cdn.xiaolincoding.com//mysql/other/format,png-20230309230622886.png){:height 195, :width 266}
	- 正常情况下，
		- 收到SYN帧创建半连接对象放入SYN队列
		- 收到ACK帧从半连接队列中去除对应半连接，创建新的连接对象放入Accept队列
		- 完成上述步骤，则连接已经**建立完毕**，等待应用调用``accept``函数从全连接队列中去除连接使用
	- ## 避免SYN攻击的方法
		- ### 调大``netdev_max_backlog``
			- 当网卡接受数据包的速度大于内核处理速度时，会有一个队列保存这些数据报，调大此队列的容量可以缓解症状
			- 此队列最大长度可通过文件``/proc/sys/net/core/netdev_max_backlog``更改
				- 修改此参数使用``sysctl -w net.core.netdev_max_backlog = 10000``修改，下同
		- ### 增大TCP半连接队列
			- 增大半连接队列长度需要同时增大如下三个参数
				- 增大``/proc/sys/net/ipv4/tcp_max_syn_backlog``
				- 增大``listen``函数中的backlog
				- 增大``/proc/sys/net/core/somaxconn``
		- ### 开启``net.ipv4.tcp_syncookies``
			- 开启此参数，系统可以在不适用半连接队列的情况下成功建立连接，相当于绕过了半连接
			- ![tcp_syncookies 应对 SYN 攻击](https://cdn.xiaolincoding.com//mysql/other/format,png-20230309230618804.png){:height 258, :width 338}
			- 工作流程为：
				- 若半连接队列已满且受到了SYN包，不会丢弃，而是根据算法计算出一个``cookie``值
				- 将此``cookie``值置于第二次握手报文的序列号字段中，发送给客户端
				- 客户端在发送ACK时会将此``cookie``也发送给服务器
				- 服务器会再次计算此`cookie`值并和客户端发来的值作比较，比较成功则认为连接有效，直接建立连接并放入全连接队列
			- 此参数的值有三种情况
				- ``0``标识关闭此功能
				- ``1``标识只有当半连接队列满时才启用此功能
				- ``2``标识无条件开启
			- `echo 1 > /proc/sys/net/ipv4/tcp_syncookies`
		- ### 减少SYN-ACK回传次数
			- 减少回传次数会加快断开连接
			- ``echo 2 > /proc/sys/net/ipv4/tcp_synack_retries``
- # TCP断开连接
	- TCP通过四次挥手的方式断开连接，双方都可主动断开连接
	- 断开后的连接，其资源将被释放
	- ![客户端主动关闭连接 —— TCP 四次挥手](https://cdn.xiaolincoding.com//mysql/other/format,png-20230309230614791.png){:height 267, :width 249}
	- 双方都需要发送一次FIN和ACK，因为TCP连接是双向的
	- **主动关闭连接的一方[[$red]]==会进入TIME_WAIT==状态**
	- ## 四次挥手的必要性
	  collapsed:: true
		- 客户端向服务器发送FIN标识客户端不再发送数据，但是**可以接受服务器还没有发送完毕的数据**
		- 服务器向客户端发送FIN之前(数据发送完毕之前)，先回一个ACK表示已经受到对方的FIN，知晓对方的关闭意愿
		- 当服务器最终也发送完所有数据之后，也会发送FIN表示数据全部发送完毕
		- 实际上服务器的ACK和FIN可以同时发送，**将四次挥手变成三次挥手**
			- 但是一般由于服务器在接收到客户端的FIN之后都需要一些时间进行相关的处理，因此不能立即做到发送完所有数据，所以服务器的ACK和FIN往往都是分开发送
		- ### 丢失第一次挥手
			- 如果第一次挥手丢失，则会触发客户端的超时重传，重传FIN帧，重发次数由``tcp_orphan-retries``控制
			- 若重传次数超过此参数，且经过了最终等待时间时候还是没有受到ACK，则直接close
		- ### 丢失第二次挥手
			- 由于ACK报文不会自动重传，因此客户端会不停重传FIN帧，要求服务器重传
			- 超过客户端的重传次数之后不再重传，最终结果和丢失第一次挥手结果类似
			- 若第二次回收**成功到达客户端**，客户端进入``FIN_WAIT_2``状态
				- 如果是使用``close``同时关闭了接受方和发送方，则等待``tcp_fin_timeout``时间之后若未收到对方的FIN将直接关闭连接，一般默认是60秒
				- 若使用的是``shutdown``仅关闭发送方，由于还保有接受能力，因此会一直处于``FIN_WAIT_2``状态直到收到对方的FIN
		- ### 丢失第三次挥手
			- 在收到客户端的FIN之后，服务器端知晓了对方的关闭意愿，但是内核并不能主动关闭连接，只有应用程序调用``close``或``shutdown``之后，服务器才会发出FIN
			- 服务器的FIN丢失和客户端的一致，重传超过``tcp_fin_out``之后直接关闭连接
		- ### 丢失第四次挥手
			- 收到第三次挥手的一方会进入一段长为**2MSL(Maximum Segment Lifetime)**的``TIME_WAIT``状态
			- 发送第三次挥手的一方进入``LAST_ACK``状态
				- 在此状态下会等待对方的ACK，若一直未等到会重传第三次握手直到上限，最终直接关闭连接
			- 发送第四次挥手的一方也会在``TIME_WAIT``之后关闭连接(每受到以此重传要求都会在重传最后的ACK之后重置``TIME_WAIT``计时器)
			- #### ``TIME_WAIT``的时间为什么是2MSL？
				- 2MSL实际上是允许报文丢失一次，比如发送给对方的最后ACK丢失，对方在等待MSL之后认为ACK已经丢失，发送FIN要求重传，到重传FIN到达客户端为止一共会消耗2MSL
				- Linux中2MSL的长度一般为60秒，在内核代码里定义了``TCP_TIMEWAIT_LEN``
					- 这个值写死在了内核里，想要改变需要重新编译
	- ## ``TIME_WAIT``状态的必要性
		- 主动发起关闭的一方才会进入此状态，其必要性体现在两方面
		- ###  防止历史数据包被后续相同四元组错误接受
			- **序列号在溢出之后会循环到0**
			- 序列号和初始序列号都不是**无限递增**，在到达最大值之后会出现**[[$red]]==回绕==**的情况
				- 发生回绕之后判断历史数据将出现一些困难
			- 不过实际上随机初始序列号的使用已经大大降低了类似情况出现的可能性，但是设计2MSL可以进一步大幅降低出错概率
		- ### 保证**被动关闭连接**的一方，能被正确关闭
			- 如果过早关闭，则服务端可能最后会收到一个RST异常关闭连接，不够优雅，甚至可能会需要重传到上限再关闭，浪费很多时间
			- 因此等待足够的一段时间也可以帮助对方正确关闭
	- ## ``TIME_WAIT``的危害
		- 主要有两种，分别是
			- 占用**系统资源**，包括文件描述符，内存资源，CPU资源，线程资源等
			- 占用**端口资**，一般可开启端口在``32768 ~ 61000``，可以通过``net.ipv4.ip_local_port_range``设定范围
		- ### 客户端
			- 客户端处于此状态的过多到占满所有端口资源，会导致短时间内无法向同一个服务器(相同IP，相同端口)再次发起连接
			- 但如果是连接一个不同IP或不同端口的服务器，那么仍然可以再次建立连接
		- ### 服务器
			- 服务器端只监听一个端口，因此``TIME_WAIT``过多基本并不会影响到客户端，但是连接过多可能还是会影响性能
		- ### 优化``TIME_WAIT``
		  collapsed:: true
			- #### 打开``net.ipv4.tcp_tw_reuse``和``net.ipv4.tcp_timstamps``
				- 用于客户端(连接发起方)
				- 开启后，在调用``connect``函数时，内核会随机找一个``TIME_WAIT``超过1秒的连接给新的连接复用
				- 第二个选项是开启TCP头部属于**可选项**的时间戳，开启之后``TIME_WAIT``状态的重要性将大大降低，因为可以通过时间戳判断过期数据
			- ### ``net.ipv4.tcp_max_tw_buckets``
				- 当系统中处于``TIME_WAIT``状态的连接超过这个值，系统会将后面的``TIME_WAIT``连接状态重置
			- #### 程序中使用``SO_LINGER``，应用强制使用RST关闭
				- ```
				  struct linger so_linger;
				  so_linger.l_onoff = 1;
				  so_linger.l_linger = 0;
				  setsockopt(s, SOL_SOCKET, SO_LINGER, &so_linger,sizeof(so_linger));
				  ```
				- 通过这种方式，在调用``close``关闭连接时会直接给对方发送RST，立即关闭连接，跳过``TIME_WAIT``状态
				- [[$red]]==**很危险**==
			- 事实上，``TIME_WAIT``是一个正常的，很有用的状态，**不应该跳过它，而应该理解它**
	- ## 服务器出现大量``TIME_WAIT``状态的原因
		- 只有主动断开连接的一方会进入此状态，因此说明服务器主动断开了很多TCP连接
		- 有**三种情况**，服务器会主动断开连接
		- ### HTTP没有使用**长连接**
			- 在HTTP/1.1之后都是默认使用长连接， 只有使用``Connection:close``字段才会不使用长连接
			- 服务器或客户端中任何一方不启用``Keep-Alive``，**都会导致服务器端主动关闭连接**
			- 解决方案很简单：**服务器和客户端都开启长连接即可**
		- ### HTTP长连接**超时**
			- 此情况可能属于正常情况，因为客户端可能确实在某一段时间会不发送任何请求
			- 可以排查网络，看是否是网络原因导致客户端的请求无法被收到
		- ### HTTP长连接的请求数量**达到上限**
			- 单个HTTP长连接**能处理的请求数量是有上限的**
			- 超过此限制，服务器会主动关闭此连接
			- 解决方式也很简单，**在QPS(Quests Per Second)较高的场景下，调高长连接请求处理上限**
				- 例如将nginx的keepalive_requests参数调高
	- ## 服务器出现大量``CLOSE_WAIT``状态的原因有哪些
		- 处于此状态说明**被被动关闭了很多链接**，但是己方(应用程序)并没有主动调用``close``函数关闭这些已经被对方关闭的程序
		- 一个普通的TCP服务器流程一般为：
			- 1.创建服务端 socket，bind 绑定端口、listen 监听端口
			- 2.将服务端 socket 注册到 epoll
			- 3.epoll_wait 等待连接到来，连接到来时，调用 accpet 获取已连接的 socket
			- 4.将已连接的 socket 注册到 epoll
			- 5.epoll_wait 等待事件发生
			- 6.对方连接关闭时，我方调用 close
		- ### 第二步没做
			- 根本没有将服务器坚挺的socket绑定到epoll，那么自然无法感知连接请求，导致客户端超时断链
			- 这种属于明显代码bug
		- ### 第三步没做
			- 没有调用accept
			- 有可能时在调用``accept``之前代码抛出了异常或卡在某个逻辑
		- ### 第四步没做
			- 在将客户端socket注册进epoll之前，代码卡在某个逻辑或抛了异常
		- ### 低六步没做
			- 同上
		- 总之，服务器出现大量``CLOSE_WAIT``通常都是代码除了问题，需要进行排查，**排查主要方向就是看为什么没有调用``close``**
- # 客户端在已经建立连接之后出现故障
	- 一个已经建立的连接，如果客户端突然宕机，则服务器端如果一直不给客户端发数据将无法感知到客户端已经丢失这个事件，**永远占用资源**
	- 因此TCP有一个**保活机制**
		- 在一定时间段内，如果TCP没有任何连接相关活动，**TCP保活机制会开始作用**，每隔一段时间发送探测报文，如果连续多个探测报文没有得到响应，则判断当前TCP连接已死亡，内核将该错误信息上报给应用层
	- Linux中可以设置保活相关参数
		- ``net.ipv4.tcp_keepalive_time``
			- 保活机制启动时间，若这个事件段内没有任何连接相关活动，则启动保活机制，单位秒
			- 默认7200
		- ``net.ipv4.tcp_keepalive_intvl``
			- 每次检查间隔的事件，单位秒
			- 默认75
		- ``net.ipv4.tcp_keepalive_probes``
			- 检测的次数，如果检测超过这个次数还收不到回应，则认为对方不可达，中断本次连接，单位秒
			- 默认9
	- 意味着在默认情况下，需要两小时11分15秒才能发现一个已经死亡的连接
	- 还需要再编程代码中开启``SO_KEEPALIVE``才能启用保活机制
	- 保活机制工作时会出现三种情况
		- **对端程序正常工作**，此情况下保活报文被正常响应，**保活时间被重置**，等待下一个保活时间到来
		- **对端主机宕机并重启**，服务器的保活报文会被重启的客户端用一个RST响应，连接重置
		- **对端主机宕机并无法回应**，会因为重发保活保活报文到达上限而关闭连接
	- TCP自带保活机制耗时较长，应用层可以自行设置超时机制，例如每当客户端做出一个请求就设置一个计时器，时间到后用回调函数关闭socket
- # Socket编程
	- ## ``accept``发生在哪一步
		- ![socket 三次握手](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4/%E7%BD%91%E7%BB%9C/socket%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B.drawio.png){:height 318, :width 405}
		- 对于**客户端来说**，`connect`函数返回是在第二次握手，即连接状态转变为ESTABLISHED之后
		- 对于**服务器来说**，``accept``成功返回是在第三次握手，即收到客户端的ACK，状态转变为ESTABLISHED之后
		- ## 客户端调用``close``主动关闭的流程
			- ![客户端调用 close 过程](https://cdn.xiaolincoding.com//mysql/other/format,png-20230309230538308.png){:height 398, :width 366}
	- ## 没有调用``accept``，能否建立TCP连接？
		- [[$blue]]==**能**==
		- ``accept``自身并不参与TCP三次握手，只负责从TCP全连接队列中取出一个已经建立连接的socket
		- ![半连接队列与全连接队列](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8D%8A%E8%BF%9E%E6%8E%A5%E5%92%8C%E5%85%A8%E8%BF%9E%E6%8E%A5/3.jpg){:height 388, :width 380}
	- ## 没有调用``listen``，能建立TCP连接吗？
		- [[$blue]]==**能**==
		- 客户端可以自己连自己形成**TCP自连接**，也可以两个客户端同时向对方发出请求(**TCP同时打开**)
		- 如果服务器端只``bind``，不``listen``，那么发向该地址和端口的请求会收到一个RST，无法正常建立连接
		-